---
layout: post
title: "Summary: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
author: Lennart Grosser
date: 2020-11-14
category: "paper-summary"
description: "A summary of the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al., 2018. BERT represents a pre-trained transformer that can be fine-tuned by training just one additional output layer, resulting in a powerful model for different tasks such as question answering or language inference. Parts of the original paper may be left out."
tags: ["machine learning", "deep learning", "attention", "self attention", "self-attention", "transformer", "bert", "bidirectionality", "bidirectional"]
---

> This post is a summary of the paper [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al., 2018](https://arxiv.org/pdf/1810.04805.pdf). BERT represents a pre-trained transformer that can be fine-tuned by training just one additional output layer, resulting in a powerful model for different tasks such as question answering or language inference. Parts of the original paper may be left out.

## Introduction
In the introduction of the paper, the authors give an overview of language model pre-training. It has been shown that pre-training language model has positive effects for downstream natural language processing tasks such as natural language inference, paraphrasing, named entity recognition and question answering.
Two diffferent strategies for using pre-trained language representation for such tasks:
1. feature-based: using the pre-trained language representation as additional features for a downstream tasks.
2. fine-tuning: fine-tuning all all pre-trained parameters given a downstream task.

Both approaches use unidirectional language models to learn _general language representations_. The authors depict that previous approaches that use the transformer architecture have only focused on creating **unidirectional** language models where each token can only attend to previous tokens. This restriction may be harmful to tasks where bidirectional information incorporation is needed, e.g. question answering.

BERT aims to solve this issue by appling a new pre-training objective , the masked language model (MLM). MLM represents randomly masking tokens and training the model to predict the masked token's vocabulary id given the surrounding tokens. This allows the language model to utilize the left and right context of a word, yielding a pre-trained **bidirectional** transformer. Further BERT uses a _next sentence prediction_, allowing to jointly learn text-pair representations.

## Related Work
The second section of the paper gives an overview of previous publications regarding unsupervised feature-based approaches, unsupervised fine-tuning approaches and transfer learning from supervised data. This section will be left out in this post but the reader is advised to have a look into the [original paper](https://arxiv.org/pdf/1810.04805.pdf).

## BERT
The third section explains BERT and its implementation. The paper describes two steps:
1. pre-training: using unlabeled data to train the model over different tasks.
2. fine-tuning: the model learns different downstream tasks using labeled data. Each downstream task fine-tunes its own pre-trained model, however all starting with the same initial parameters.

### Model Architecture
BERT's architecture is based on the vanilla transformer architecture, described [here](https://lenngro.github.io/paper-summary/2020/11/07/Attention-Is-All-You-Need/). Two different configurations of the original transformer architecture are examined:
1. $$BERT_{BASE}$$: $$L=12$$, $$H=768$$, $$A=12$$, Total Parameters=110M
2. $$BERT_{LARGE}$$: $$L=24$$, $$H=1024$$, $$A=16$$, Total Parameters=340M
where $$L$$ is the number of layers (transformer blocks), $$H$$ is the hidden size and $$A^3$$ is the number of attention heads.

### Input/Output Representations
To cover different downstream tasks, BERT uses a token sequence to represent either a single sentence or a pair of sentences (for question answering). The authors rely on WordPiece embeddings with a vocabulary of 30000 tokens. Every token sequence starts with a special classification token ([CLS]). Two sentences that are concatenated into a single sequence are separated with a sepcial token ([SEP]). Further, an additional embedding is learned that indicates whether a token belongs to the first or second sentence.
A token's input representation is created by summing the token embedding within the sentence embedding, the segment embedding (the previously described sentence affiliation) and the positional embedding.

### Pre-Training
#### Masked Language Model (MLM)
The authors motivate the bidirectionality of the model by arguing that intuitively a bidirectional model should be more powerful than either a left-to-right or a right-to-left model. BERT achieves bidirectionality by masking 15% of the tokens (actually embeddings) in a sentence by replacing them with a special token ([MASK]) and then predicting those masked tokens.

Masked tokens appear only during pre-training, not during fine-tuning. To prevent possible problems that might occur because of this mismatch, tokens that are selected for masking and therefore prediction are only replaced by the special token in 80% of the time, by a random token in 10% of the time and not replaced at all in another 10% of the time.

#### Next Sentence Prediction (NSP)
An important downstream task for Question Answering and Natural Language Inference is the prediction of the next sentence given the current sentence. The authors train the model on binarized next sentence prediction: 50% of the time the true label of the current prediction sample is true (is actually the next sentence) and 50% it's false (is not the next sentence).

#### Pre-training data
BERT uses the BooksCorpus (800M words) and English Wikipedia (2500M Words, only text passages).


### Fine-tuning
The authors fine-tune the pre-trained BERT model on 11 NLP tasks. For each task, the task-specific inputs and outputs into BERT and fine-tune all the parameters end-to-end.  

## Experiments
The experiment section is skipped. Please refer to the [original paper](https://arxiv.org/pdf/1810.04805.pdf).

## Ablation Studies
Ablation studies investigate aspects of BERT and how they influence the predictive performance of the model.

### Effect of Pre-training Tasks
To show how importance the deep bidirectionality of BERT is, the authors compare two models that were trained using the same pre-training data, fine-tuning scheme and hyperparameters.

1. No NSP: A bidirectional model trained with the masked language model but without the next sentence prediction.
2. LTR & No NSP: A left-context-only model which is trained without the MLM but a left-to-right language model as well as without the next-sentence prediction.

Removing the NSP task significantly reduces performance on three tasks. Removing the bidirectionality of the language model (--> LTR) peforms worse than the MLM model on all tasks.

### Effect of Model Size

As expected, increasing the model size improves BERT's performance: increasing the number of layers, the maximum sequence length or the number of attention heads all increases the accuracy on different tasks.

### Feature-based Approach with BERT
Compared to a fine-tuning approach to downstream tasks (adding a classification layer to BERT and proceeding to further train the pre-trained model as classification model), one can also use the learned features of BERT as input to another model (extracting the features for a given input, then using these features as input to a (smaller) classification model).
The results show that depending on the specific implementation of the feature extraction, the feature-based approach reaches almost the same F1 score on the test data as the fine-tuning approach. That means BERT qualifies for fine-tuning and feature-based approaches.
